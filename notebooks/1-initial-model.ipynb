{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.trace_guide import TraceGuide\n",
    "from src.models.initial_model import InitialModel\n",
    "from src.data.datasets import FloorDataset\n",
    "from src.utils import object_to_markdown\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "site_id = \"5a0546857ecc773753327266\"\n",
    "floor_id = \"B1\"\n",
    "\n",
    "floor_data = FloorDataset(site_id, floor_id, wifi_threshold=200, sampling_interval=100)\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial model, we are only looking at including the wifi signals of the model. We have implemented the model in `src/models/initial_model.py` as a method of the `InitialModel` class which also holds various model attributes and the variational parameters. The source code of the generative model can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(object_to_markdown(InitialModel.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we wanted the prior on the wifi locations and the initial trace positions to be uniform over the floor area. However we ran into domain problems, when attempting to define the variational posterior. We therefore decided to relax the prior a bit to a normal distribution with the same mean and variance.\n",
    "\n",
    "We use mini batches of padded sequences during training, so we also need to provide a mask of padded observations. This mask also serves as a mask of missing observations, as these should essentially be handled identically. We are using the `torch` data loader protocol, and the specific implementation can be seen in `src/data/datasets.py`. Below, we extract a mini_batch of 4 traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = floor_data[torch.arange(16, 20)]\n",
    "mini_batch_index = mini_batch[0]\n",
    "mini_batch_length = mini_batch[1]\n",
    "mini_batch_time = mini_batch[2]\n",
    "mini_batch_position = mini_batch[3]\n",
    "mini_batch_position_mask = mini_batch[4]\n",
    "mini_batch_wifi = mini_batch[5]\n",
    "mini_batch_wifi_mask = mini_batch[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "initial_model = InitialModel(floor_data)\n",
    "x, wifi_location = initial_model.model(*mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see the corresponding samples. Since they are essentially random walks (only conditioned on the previous observation), they dont really resemble the trace data in structure yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(*wifi_location.T, marker=\".\", color=\"grey\", label=\"wifi locations\")\n",
    "plt.plot(*x.T, label=[f\"trace {i}\" for i in range(x.shape[0])])\n",
    "plt.xlim(x[...,0].min()-5, x[...,0].max()+5)\n",
    "plt.ylim(x[...,1].min()-5, x[...,1].max()+5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational distribution\n",
    "In the model, we sometimes observe $\\hat {\\boldsymbol x}_i $ as an observation of a latent variable $\\boldsymbol x_i$. \n",
    "In order to use variational inference, we need an approximate posterior for the latent variable $\\boldsymbol x_i$. \n",
    "Since the latent position is sampled quite frequently, if we just use a mean field approximation, we could end up training a lot of variational parameters. \n",
    "Instead, with inspiration from [pyro's Deep Markov Model example](https://pyro.ai/examples/dmm.html), we use a parameterized function  $\\tilde { \\boldsymbol  x}_\\theta : \\mathbb{R}_{\\geq 0} \\mapsto \\mathbb{R}^2$ for each trace to hopefully follow the latent path. We can then construct the variational posterior with variational parameters $\\theta$ as \n",
    "$$\n",
    "q(\\boldsymbol x_i | \\theta) = \\mathcal N (\\boldsymbol x_i \\mid \\tilde { \\boldsymbol x}_\\theta(t_i), s \\mathbf I )\n",
    "$$\n",
    "where the diagonal variance $s$ is also a variational parameter.\n",
    "\n",
    "We want a class of functions $\\tilde{\\boldsymbol x}_\\theta$ that is quite flexible, yet doesn't have too many parameters to fit. We decided to look at functions of the form: \n",
    "$$\n",
    "\\tilde{\\boldsymbol x}_\\theta(t) = \\boldsymbol\\beta_0 + \\sum_{i=1}^{10} \\boldsymbol \\alpha_i h( t - \\boldsymbol \\gamma_i).\n",
    "$$\n",
    "Where $h$ is the softplus function.\n",
    "This has been implemented as the `TraceGuide` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(object_to_markdown(TraceGuide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "tg = TraceGuide(time_min_max=(0, 10), n=5)\n",
    "tt = torch.linspace(0, 15, 100)\n",
    "with torch.autograd.no_grad():\n",
    "    basis_functions = torch.nn.functional.softplus(tt.view(-1, 1, 1) - tg.time_offset)\n",
    "    basis_w_coeffs = basis_functions * tg.coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the parameters $\\theta=(\\beta_0, \\alpha_1, \\dots, \\alpha_{10}, \\gamma_1, \\dots, \\gamma_{10}, )$, to obtain quite flexible paths in $\\mathbb{R}^2$. Below the basis for one of the dimensions is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(10,10))\n",
    "\n",
    "axes[0].plot(tt, basis_functions[..., 0])\n",
    "axes[0].set_title(\"$h(t-\\\\gamma_i)$\")\n",
    "axes[1].plot(tt, basis_w_coeffs[..., 0])\n",
    "axes[1].set_title(\"$\\\\alpha_i \\\\cdot h(t-\\\\gamma_i)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a few traces for randomly generated basis coefficients can be seen. Also shown is the corresponding standard deviation, in this case initialized as $\\exp[0] = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(126)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(5):\n",
    "    tg = TraceGuide(time_min_max=(0, 10), n=5)\n",
    "    with torch.autograd.no_grad():\n",
    "        location, scale = tg(tt.view(-1,1))\n",
    "    plt.plot(*location.T, \"--\", color=f\"C{i}\")\n",
    "    ax = plt.gca()\n",
    "    for j in range(19, len(tt), 10):\n",
    "        ax.plot(*location[j], \"x\", color=f\"C{i}\")\n",
    "        ax.add_patch(plt.Circle(location[j], scale, fill=False, color=f\"C{i}\"))\n",
    "plt.axis(\"equal\")   \n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a mean field approximation for the other variational distributions eg. wifi locations and signal strengths. The corresponding guide function can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(object_to_markdown(InitialModel.guide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Due to the size of the model / the number of parameters, the model is trained using the `src/models/initial_models.py` script. This allowed us to easily use the DTU HPC cluster for training. Below, we load a checkpoint of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def print_a():\n",
    "    print(\"a\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../checkpoints/initial_model.pt\")\n",
    "initial_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the training loss over the course of training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(checkpoint[\"loss_history\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (Negative ELBO)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04d12789f47ca115d7ef1a39d997101d29a535d995c3316df46e69717c877462"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "518b9198cbe31b7727d65f8d03e048c87d2d5bd656f1a5e3ba6942f89a1b6181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
